{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyND/dYS3YYb8aewhwJXbchs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RsAgfezp-yyY","executionInfo":{"status":"ok","timestamp":1761155046524,"user_tz":-330,"elapsed":9832,"user":{"displayName":"Ashwal Shanbhag","userId":"10390027574939309166"}},"outputId":"2f727256-bd83-4dd2-d5a1-dca71f1670b0"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["from datasets import load_dataset\n","\n","ds = load_dataset(\"lucadiliello/newsqa\")"]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Input, Embedding\n","from tensorflow.keras.models import Model\n","import re\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"NcW_Rm2N-5S-","executionInfo":{"status":"ok","timestamp":1761155054425,"user_tz":-330,"elapsed":7896,"user":{"displayName":"Ashwal Shanbhag","userId":"10390027574939309166"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Parameters\n","voc_size = 10000\n","sent_length = 50  #consider largest sentence in NewsQA , tried to use largest value , ensuring ram to not crash\n","question_len = 30 #considered as\n","dim = 100 #each word is converted into 100 dim vector"],"metadata":{"id":"zRe-zHbi-9Xu","executionInfo":{"status":"ok","timestamp":1761155054467,"user_tz":-330,"elapsed":18,"user":{"displayName":"Ashwal Shanbhag","userId":"10390027574939309166"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Dataset\n","corpus = [sample[\"context\"] for sample in ds[\"train\"]]\n","questions = [sample[\"question\"] for sample in ds[\"train\"]]"],"metadata":{"id":"1KzkBs5v_AE0","executionInfo":{"status":"ok","timestamp":1761155068948,"user_tz":-330,"elapsed":14474,"user":{"displayName":"Ashwal Shanbhag","userId":"10390027574939309166"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Clean text\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # removing unwanted punctuation marks\n","    text = re.sub(r\"\\s+\", \" \", text).strip() #removing extra spaces\n","    return text\n","\n","corpus = [clean_text(text) for text in corpus]\n","questions = [clean_text(text) for text in questions]"],"metadata":{"id":"UfEkhsLi_EcS","executionInfo":{"status":"ok","timestamp":1761155088803,"user_tz":-330,"elapsed":19848,"user":{"displayName":"Ashwal Shanbhag","userId":"10390027574939309166"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["In the previous task I had used one-hot encoding to create word embedding but since i have to do it on very large dataset using tokens . One hot encoding would create very large vectors for each word , wereas  tokenization assigns a unique integer (token) to each word, and these tokens are  mapped  to vectors using an  pre-trained embeddings (like GloVe or Word2Vec or BERT )."],"metadata":{"id":"hqP-2_eF_IEr"}},{"cell_type":"code","source":["# Tokenizer\n","all_texts = corpus + questions\n","tokenizer = Tokenizer(num_words=voc_size, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(all_texts)\n","\n","context_seq = tokenizer.texts_to_sequences(corpus)\n","question_seq = tokenizer.texts_to_sequences(questions)\n","#padding done to make all vectorrs of same dim\n","context_seq = pad_sequences(context_seq, maxlen=sent_length, padding='post')\n","question_seq = pad_sequences(question_seq, maxlen=question_len, padding='post')"],"metadata":{"id":"18yar31i_Klq","executionInfo":{"status":"ok","timestamp":1761155124523,"user_tz":-330,"elapsed":35699,"user":{"displayName":"Ashwal Shanbhag","userId":"10390027574939309166"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Shared embedding layer\n","context_input = Input(shape=(sent_length,))\n","question_input = Input(shape=(question_len,))\n","\n","embedding_layer = Embedding(input_dim=voc_size, output_dim=dim, mask_zero=True)\n","\n","context_emb = embedding_layer(context_input)\n","question_emb = embedding_layer(question_input)\n","\n","model = Model(inputs=[context_input, question_input], outputs=[context_emb, question_emb])\n","model.compile(optimizer='adam', loss='mse')\n","\n","# Get embeddings\n","context_embeddings, question_embeddings = model.predict([context_seq, question_seq])\n","print(\"Context embeddings shape:\", context_embeddings.shape)\n","print(\"Question embeddings shape:\", question_embeddings.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uGtcIZVC_PKz","executionInfo":{"status":"ok","timestamp":1761155134123,"user_tz":-330,"elapsed":9579,"user":{"displayName":"Ashwal Shanbhag","userId":"10390027574939309166"}},"outputId":"c02e7830-2c78-47e5-9810-bb48db9604ab"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m2318/2318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n","Context embeddings shape: (74160, 50, 100)\n","Question embeddings shape: (74160, 30, 100)\n"]}]},{"cell_type":"code","source":["#Extract the learned embedding weights\n","embedding_matrix = embedding_layer.get_weights()[0]  # shape: (voc_size, dim)\n","\n","word_index = tokenizer.word_index\n","\n","# Saving part\n","data = []\n","for word, idx in word_index.items():\n","    if idx < voc_size:\n","        embedding_vector = embedding_matrix[idx]\n","        data.append([word] + embedding_vector.tolist())\n","\n","# Convert to DataFrame\n","df = pd.DataFrame(data)\n","df.to_csv(\"word_embeddings.csv\", index=False, header=False)  # no header, no index\n","\n","print(\"Saved word embeddings to word_embeddings.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PziUCAT6_RM8","executionInfo":{"status":"ok","timestamp":1761155136264,"user_tz":-330,"elapsed":2136,"user":{"displayName":"Ashwal Shanbhag","userId":"10390027574939309166"}},"outputId":"a69137fb-d241-4a28-8b04-5e35b9b96b98"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved word embeddings to word_embeddings.csv\n"]}]}]}